import dependencies
import tweepy
from tweepy import OAuthHandler
from tweepy.streaming import StreamListener
import json
from tweepy import Stream
import time
import datetime 
import requests
import bs4
from bs4 import BeautifulSoup
import re
import pandas as pd
from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer
from selenium import webdriver
from selenium.webdriver.common.keys import Keys
import urllib.parse
import sys
analyzer = SentimentIntensityAnalyzer()


base_url = 'http://twitter.com/'
query = "BillGates"

r = requests.get(base_url+query)
soup = BeautifulSoup(r.text,'html.parser')

with open('bg_save',mode='w',encoding='utf-8')as f:
    f.write(r.text)

contentPTags = soup.body.findAll('p')

contentParagraphs  = []
for pTag in contentPTags:
    contentParagraphs.append(re.sub(r'\[\d+\]','',pTag.text))
contentParagraphsDF = pd.DataFrame({'paragraph-text' : contentParagraphs})
#print(contentParagraphsDF)

otherPageUrls = []
for paragraphNum, pTag in enumerate(contentPTags):
    taglinks = pTag.findAll('a',href=re.compile('.twitter.'),class_=False)
    for aTag in taglinks:
        relurl = aTag.get('href')
        linkText = aTag.text 
        otherPageUrls.append((urllib.parse.urljoin(base_url,relurl),paragraphNum,linkText,))
#print(otherPageUrls[:10])

contentParagraphsDF['source'] = [base_url+query]*len(contentParagraphsDF['paragraph-text'])
contentParagraphsDF['paragraph-number']=range(len(contentParagraphsDF['paragraph-text']))
print(contentParagraphsDF)
contentParagraphsDF['source-paragraph-number'] = [None] * len(contentParagraphsDF['paragraph-text'])
contentParagraphsDF['source-paragraph-text'] = [None] * len(contentParagraphsDF['paragraph-text'])

def getTextFromTwitterPage(targetURL, sourceParNum, sourceText):
    parsDict = {'source' : [], 'paragraph-number' : [], 'paragraph-text' : [], 'source-paragraph-number' : [],  'source-paragraph-text' : []}
    r = requests.get(targetURL)
    soup = bs4.BeautifulSoup(r.text, 'html.parser')
    for parNum, pTag in enumerate(soup.body.findAll('p')):
        parsDict['paragraph-text'].append(re.sub(r'\[\d+\]', '', pTag.text))
        parsDict['paragraph-number'].append(parNum)
        parsDict['source'].append(targetURL)
        parsDict['source-paragraph-number'].append(sourceParNum)
        parsDict['source-paragraph-text'].append(sourceText)
    return pd.DataFrame(parsDict)
for urlTuple in otherPageUrls[:3]:
    contentParagraphsDF = contentParagraphsDF.append(getTextFromTwitterPage(*urlTuple),ignore_index=True,sort=False)
contentParagraphsDF

#API

stdoutOrigin=sys.stdout 
sys.stdout = open("out.txt", "w")
csv_to_save = "Twitter_data_table.csv"

url = "http://api.twitter.com/1.1/search/tweets.json"
API_key = "dAcQ3xOznkH4weiHNcOCtJlBU"
API_secret = "hXLhDenPi0BXLpiEf21726R6VnIqCdwEF2Bec1If3EE4iU9TIr"
Access_token = "1219979323945443328-xk9kQNKCCtAXN3gk4pOy86YO3cgW4i"
Access_token_secret = "vnn7ylL8qHUwB27nGi7POhFidO9hjWbMDPKan2fDS2CSY"

tracklist = ['#COVID-19','#coronavirus']
tweet_count = 0
n_tweets = 500

class stdOutListener(StreamListener):
    def on_data(self,data):
        global tweet_count
        global n_tweets
        global stream
        if tweet_count < n_tweets:
            print(data)
            tweet_count += 1
            return True
        else:
            stream.disconnect()

    def on_error(self,status):
        print(status)

if __name__ == '__main__':
    l = stdOutListener()
    auth = OAuthHandler(API_key,API_secret)
    auth.set_access_token(Access_token,Access_token_secret)
    stream = Stream(auth,l)
    stream.filter(track=tracklist)

sys.stdout.close()
sys.stdout=stdoutOrigin
3
tweets_data_path = 'out.txt'
tweets_data = []
tweets_file = open(tweets_data_path,'r')
for line in tweets_file:
    try:
        tweet = json.loads(line)
        tweets_data.append(tweet)
    except:
        continue

tweets = pd.DataFrame()
tweets['text'] = list(map(lambda tweet : tweet['text'],tweets_data))
tweets['Username'] = list(map(lambda tweet:tweet['user']['screen_name'],tweets_data))
tweets['Timestamp'] = list(map(lambda tweet:tweet['created_at'],tweets_data))
tweets['location'] = list(map(lambda tweet:tweet['user']['location'],tweets_data))
print(tweets.head)
